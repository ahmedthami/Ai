{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TextClassification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPG2pgJZPsOb5f1bcMX+BtQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ahmedthami/Ai/blob/main/TextClassification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "# print(os.listdir(os.path.join(os.getcwd() , 'gdrive/MyDrive/aclImdb')))\n",
        "\n",
        "\n",
        "def load_imdb_sentiment_analysis_dataset(data_path= os.path.join(os.getcwd(), 'gdrive/MyDrive/aclImdb'), seed=123):\n",
        "    \"\"\"Loads the IMDb movie reviews sentiment analysis dataset.\n",
        "\n",
        "    # Arguments\n",
        "        data_path: string, path to the data directory.\n",
        "        seed: int, seed for randomizer.\n",
        "\n",
        "    # Returns\n",
        "        A tuple of training and validation data.\n",
        "        Number of training samples: 25000\n",
        "        Number of test samples: 25000\n",
        "        Number of categories: 2 (0 - negative, 1 - positive)\n",
        "\n",
        "    # References\n",
        "        Mass et al., http://www.aclweb.org/anthology/P11-1015\n",
        "\n",
        "        Download and uncompress archive from:\n",
        "        http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "    \"\"\"\n",
        "    imdb_data_path = data_path\n",
        "\n",
        "    # Load the training data\n",
        "    train_texts = []\n",
        "    train_labels = []\n",
        "    for category in ['pos', 'neg']:\n",
        "        train_path = os.path.join(imdb_data_path, 'train', category)\n",
        "        for fname in sorted(os.listdir(train_path)):\n",
        "            if fname.endswith('.txt'):\n",
        "                with open(os.path.join(train_path, fname)) as f:\n",
        "                    train_texts.append(f.read())\n",
        "                train_labels.append(0 if category == 'neg' else 1)\n",
        "\n",
        "    # Load the validation data.\n",
        "    test_texts = []\n",
        "    test_labels = []\n",
        "    for category in ['pos', 'neg']:\n",
        "        test_path = os.path.join(imdb_data_path, 'test', category)\n",
        "        for fname in sorted(os.listdir(test_path)):\n",
        "            if fname.endswith('.txt'):\n",
        "                with open(os.path.join(test_path, fname)) as f:\n",
        "                    test_texts.append(f.read())\n",
        "                test_labels.append(0 if category == 'neg' else 1)\n",
        "\n",
        "    # Shuffle the training data and labels.\n",
        "    random.seed(seed)\n",
        "    random.shuffle(train_texts)\n",
        "    random.seed(seed)\n",
        "    random.shuffle(train_labels)\n",
        "\n",
        "    return ((train_texts, np.array(train_labels)),\n",
        "            (test_texts, np.array(test_labels)))\n",
        "\n",
        "\n",
        "data = load_imdb_sentiment_analysis_dataset()\n",
        "(x_train, y_train), (x_test, y_test) = data\n"
      ],
      "metadata": {
        "id": "1_dUnDXQwIp7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc9ac257-35ce-4df2-ef52-8826f8e1bfea"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def get_number_of_words(sample_text):\n",
        "  #return median of words per sample\n",
        "  words = [len(s.split()) for s in sample_text]\n",
        "  return np.median(words)\n",
        "\n",
        "\n",
        "def rep_graph(sample_text):\n",
        "  plt.hist([len(s) for s in sample_text], 50)\n",
        "  plt.xlabel(\"number or samples\")\n",
        "  plt.ylabel(\"len of words\")\n",
        "  plt.title(\"distribution\")\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "print(len(x_test))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zAGvP8Y2wqRo",
        "outputId": "cbcfbefe-4de8-4195-d4cb-5292a1566dcc"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_selection import SelectKBest\n",
        "from sklearn.feature_selection import f_classif\n",
        "\n",
        "\n",
        "#bag of words approach\n",
        "\n",
        "NGRAM_RANGE = (1,2)\n",
        "TOP_K = 20000\n",
        "TOKEN_MODE = 'word'\n",
        "MIN_DOC_FREQ = 2\n",
        "\n",
        "def vectorize_texts(train_data, train_labels, validation_data):\n",
        "  #using tfIDTokenizer\n",
        "\n",
        "  kwargs = {\n",
        "      'anaylser' : TOKEN_MODE,\n",
        "      'ngram_range': NGRAM_RANGE,\n",
        "      'min_df': MIN_DOC_FREQ,\n",
        "      'dtype': 'int32',\n",
        "      'decoder': 'replace',\n",
        "      'strip_accents': 'unicode'\n",
        "\n",
        "  }\n",
        "\n",
        "  vectorizer = TfidfVectorizer(**kwargs)\n",
        "\n",
        "  #learn vocabulary and return idf-tf document matrix\n",
        "  x_train = vectorizer.fit_transform(train_data)\n",
        "  # return document based matrix\n",
        "  x_validation = vectorizer.transform(validation_data)\n",
        "\n",
        "  #select top K-features\n",
        "\n",
        "  #selector takes two arrays X and Y , \n",
        "  #SelectKBest(score_func = default ==> f_classif )\n",
        "  selector = SelectKBest(score_func= 'f_classif', k= min(TOP_K, x_train.shape[1]))\n",
        "  selector.fit(x_train, train_labels)\n",
        "  x_val = selector.transform(x_validation).astaype('float32')\n",
        "  x_train = selector.transform(x_train).astype('float32')\n",
        "\n",
        "\n",
        "  return x_train, x_val\n",
        "\n"
      ],
      "metadata": {
        "id": "nZDCPqjMnfit"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.python.keras.engine.sequential import Sequential\n",
        "#defining the model\n",
        "\n",
        "from tensorflow.keras import models\n",
        "from tensorflow.keras.activations import softmax, relu\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "\n",
        "def get_last_layers_params(num_classes):\n",
        "  if num_classes > 2:\n",
        "    act_function = 'softmax'\n",
        "    units = num_classes\n",
        "  else:\n",
        "    act_function = 'sigmoid'\n",
        "    units = 1\n",
        "  return units , act_function\n",
        "\n",
        "def MyModel(layers, all_units , classes_num, input_shap , dropout_rate):\n",
        "  last_units , act_function = get_last_layers_params(classes_num)\n",
        "  model = Sequential()\n",
        "  model.add(Dropout(rate = dropout_rate, input_shape = input_shap))\n",
        "  for _ in range(layers-1):\n",
        "    model.add(Dense(units = all_units , activation= 'relu', kernel_initializer= \"he_uniform\"))\n",
        "    model.add(Dropout(rate = dropout_rate))\n",
        "  model.add(Dense(last_units, activation= act_function))\n",
        "  return model"
      ],
      "metadata": {
        "id": "mqy4Yc6h9ewR"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def training_model(data, input_shape, epochs = 12, batch_size = 200, num_classes = 2, all_units = 64, dropout_rate = 0.2, layers = 2, learning_rate= 1e-3):\n",
        "\n",
        "  (x_train, train_label), (x_validation, val_labels) = data\n",
        "  train_data , val_data = vectorize_texts(x_train, train_label, x_validation)\n",
        "  #evaluation?\n",
        "  model = MyModel(layers, all_units, num_classes, input_shape, dropout_rate)\n",
        "  opt = tensorflow.keras.optimizer.adam(lr = learning_rate)\n",
        "  model.compile(optimizer= opt,loss= 'binary_crossentropy', metrics = ['accuracy'])\n",
        "  history = model.fit(train_data, train_label,batch_size = batch_size,validation_data = (val_data, val_labels) ,epochs = epochs, verbose = 0)\n",
        "\n",
        "\n",
        "  \n",
        "\n"
      ],
      "metadata": {
        "id": "OBMcYrmM9lpi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}